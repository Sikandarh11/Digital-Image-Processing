{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:24:29.179885Z",
     "start_time": "2025-05-09T14:23:59.412005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to your video file\n",
    "video_path = \"PXL_20250325_043922504.TS.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "# For FPS calculation\n",
    "prev_time = time.time()\n",
    "frame_count = 0\n",
    "fps = 0\n",
    "\n",
    "def is_dashed(lines, threshold=20):\n",
    "    if len(lines) < 4:\n",
    "        return False\n",
    "    lines = sorted(lines, key=lambda p: p[1])  # sort by y\n",
    "    gaps = []\n",
    "    for i in range(0, len(lines) - 2, 2):\n",
    "        y1 = lines[i][1]\n",
    "        y2 = lines[i + 2][1]\n",
    "        gaps.append(abs(y2 - y1))\n",
    "    if len(gaps) == 0:\n",
    "        return False\n",
    "    avg_gap = sum(gaps) / len(gaps)\n",
    "    return avg_gap > threshold\n",
    "\n",
    "\n",
    "# Corrected the __init__ method (previously it was _init_)\n",
    "class LaneStateMachine:\n",
    "    def __init__(self, stability_threshold=5):\n",
    "        self.state = \"No Lane Detected\"\n",
    "        self.counters = defaultdict(int)\n",
    "        self.threshold = stability_threshold\n",
    "\n",
    "    def update(self, new_state):\n",
    "        for key in self.counters:\n",
    "            if key != new_state:\n",
    "                self.counters[key] = 0\n",
    "\n",
    "        self.counters[new_state] += 1\n",
    "\n",
    "        # If new_state is consistent for N frames, update state\n",
    "        if self.counters[new_state] >= self.threshold:\n",
    "            if new_state != self.state:\n",
    "                self.state = new_state\n",
    "                # Reset all counters after transition\n",
    "                for key in self.counters:\n",
    "                    self.counters[key] = 0\n",
    "        return self.state\n",
    "\n",
    "\n",
    "lane_state_machine = LaneStateMachine(stability_threshold=5)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (960, 540))\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Lane Detection ROI (lower half)\n",
    "    lower_half = frame[height // 2:, :]\n",
    "    crop_margin = int(0.15 * width)\n",
    "    lower_half_cropped = lower_half[:, crop_margin:width - crop_margin]\n",
    "\n",
    "    lab = cv2.cvtColor(lower_half_cropped, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    lab = cv2.merge((cl, a, b))\n",
    "    lower_half_cropped = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    hsv_lane = cv2.cvtColor(lower_half_cropped, cv2.COLOR_BGR2HSV)\n",
    "    lower_yellow = np.array([15, 30, 30])\n",
    "    upper_yellow = np.array([40, 255, 255])\n",
    "    lane_mask = cv2.inRange(hsv_lane, lower_yellow, upper_yellow)\n",
    "\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    lane_mask = cv2.morphologyEx(lane_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    lane_mask = cv2.morphologyEx(lane_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    yellow_segment = cv2.bitwise_and(lower_half_cropped, lower_half_cropped, mask=lane_mask)\n",
    "    gray = cv2.cvtColor(yellow_segment, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blur, 100, 200)\n",
    "\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=50, minLineLength=50, maxLineGap=30)\n",
    "    line_image = np.zeros_like(frame)\n",
    "    left_lines, right_lines = [], []\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            x1 += crop_margin\n",
    "            x2 += crop_margin\n",
    "            y1 += height // 2\n",
    "            y2 += height // 2\n",
    "\n",
    "            if x2 - x1 == 0:\n",
    "                continue\n",
    "\n",
    "            slope = (y2 - y1) / (x2 - x1)\n",
    "\n",
    "            if abs(slope) < 0.5:\n",
    "                continue\n",
    "\n",
    "            if slope < 0:\n",
    "                left_lines.extend([(x1, y1), (x2, y2)])\n",
    "            else:\n",
    "                right_lines.extend([(x1, y1), (x2, y2)])\n",
    "\n",
    "    left_dashed = is_dashed(left_lines)\n",
    "    right_dashed = is_dashed(right_lines)\n",
    "\n",
    "    if left_lines or right_lines:\n",
    "        plot_y = np.linspace(height // 2, height - 1, num=height // 2)\n",
    "\n",
    "        if left_lines:\n",
    "            left_points = np.array(left_lines)\n",
    "            left_fit = np.polyfit(left_points[:, 1], left_points[:, 0], 1)\n",
    "            left_x = left_fit[0] * plot_y + left_fit[1]\n",
    "            for x, y in zip(left_x, plot_y):\n",
    "                cv2.circle(line_image, (int(x), int(y)), 2, (255, 0, 0), -1)\n",
    "\n",
    "        if right_lines:\n",
    "            right_points = np.array(right_lines)\n",
    "            right_fit = np.polyfit(right_points[:, 1], right_points[:, 0], 1)\n",
    "            right_x = right_fit[0] * plot_y + right_fit[1]\n",
    "            for x, y in zip(right_x, plot_y):\n",
    "                cv2.circle(line_image, (int(x), int(y)), 2, (0, 0, 255), -1)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Obstacle Detection with color classification\n",
    "    obstacle_crop_margin = int(0.35 * width)\n",
    "    obstacle_roi = frame[:, obstacle_crop_margin:width - obstacle_crop_margin]\n",
    "    hsv_obstacle = cv2.cvtColor(obstacle_roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    color_ranges = [\n",
    "        (np.array([0, 0, 0]), np.array([180, 255, 50]), 'Black'),\n",
    "        (np.array([90, 50, 50]), np.array([130, 255, 255]), 'Blue'),\n",
    "        (np.array([0, 70, 50]), np.array([10, 255, 255]), 'Red'),\n",
    "        (np.array([160, 70, 50]), np.array([180, 255, 255]), 'Red')\n",
    "    ]\n",
    "\n",
    "    combined_mask = np.zeros_like(hsv_obstacle[:, :, 0])\n",
    "    color_mask_map = []\n",
    "\n",
    "    for lower, upper, color_name in color_ranges:\n",
    "        mask = cv2.inRange(hsv_obstacle, lower, upper)\n",
    "        color_mask_map.append((mask, color_name))\n",
    "        combined_mask = cv2.bitwise_or(combined_mask, mask)\n",
    "\n",
    "    combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    obstacle_detected = False\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 500 and area < 750:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            center_x = x + w // 2\n",
    "            x_global = x + obstacle_crop_margin\n",
    "            center_x_global = center_x + obstacle_crop_margin\n",
    "\n",
    "            if (width // 4 < center_x_global < 3 * width // 4):\n",
    "                mask_roi = np.zeros_like(combined_mask)\n",
    "                cv2.drawContours(mask_roi, [cnt], -1, 255, -1)\n",
    "                mean_hsv = cv2.mean(hsv_obstacle, mask=mask_roi)[:3]\n",
    "\n",
    "                detected_color_name = \"Unknown\"\n",
    "                for lower, upper, color_name in color_ranges:\n",
    "                    if np.all(lower <= mean_hsv) and np.all(mean_hsv <= upper):\n",
    "                        detected_color_name = color_name\n",
    "                        break\n",
    "\n",
    "                obstacle_detected = True\n",
    "                cv2.rectangle(frame, (x_global, y), (x_global + w, y + h), (0, 0, 255), 2)\n",
    "                cv2.putText(frame, f\"Obstacle: {detected_color_name}\", (x_global, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow('Obstacle ROI', obstacle_roi)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Decision Making - Raw State\n",
    "    if obstacle_detected:\n",
    "        current_decision = \"STOP - Obstacle Ahead\"\n",
    "    elif right_dashed and not left_dashed:\n",
    "        current_decision = \"Right Turn Possible – Dashed Line\"\n",
    "    elif left_dashed and not right_dashed:\n",
    "        current_decision = \"Left Turn Possible – Dashed Line\"\n",
    "    elif not left_lines and not right_lines:\n",
    "        current_decision = \"No Lane Detected\"\n",
    "    else:\n",
    "        current_decision = \"Go Straight\"\n",
    "\n",
    "    # Use state machine to stabilize the decision\n",
    "    direction_text = lane_state_machine.update(current_decision)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Final Display\n",
    "    combined_lower = cv2.addWeighted(lower_half_cropped, 0.7, cv2.cvtColor(lane_mask, cv2.COLOR_GRAY2BGR), 0.3, 0)\n",
    "    combined_lower = cv2.addWeighted(combined_lower, 0.9, cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR), 0.4, 0)\n",
    "\n",
    "    combined_full = frame.copy()\n",
    "    combined_full[height // 2:, crop_margin:width - crop_margin] = combined_lower\n",
    "    output = cv2.addWeighted(combined_full, 1, line_image, 0.6, 0)\n",
    "\n",
    "    cv2.rectangle(output, (20, 20), (620, 80), (0, 0, 0), -1)\n",
    "    cv2.putText(output, direction_text, (30, 65), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count >= 10:\n",
    "        curr_time = time.time()\n",
    "        fps = frame_count / (curr_time - prev_time)\n",
    "        prev_time = curr_time\n",
    "        frame_count = 0\n",
    "\n",
    "    cv2.putText(output, f\"FPS: {fps:.2f}\", (700, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    print()\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Lane Area', line_image)\n",
    "    cv2.imshow('Final Output', output)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "a5a345a8d882f864",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def detect_objects_on_road(frame):\n",
    "    global previous_frame\n",
    "    height, width = frame.shape[:2]\n",
    "    roi_polygon = np.array([[(100, height), (2200, height), (1000, 260), (700, 260)]])\n",
    "    roi_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.fillPoly(roi_mask, roi_polygon, 255)\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bitwise_and(gray, gray, mask=roi_mask)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    if previous_frame is None:\n",
    "        previous_frame = blur\n",
    "        return frame, False\n",
    "\n",
    "    diff = cv2.absdiff(previous_frame, blur)\n",
    "    _, thresh = cv2.threshold(diff, 60, 255, cv2.THRESH_BINARY)\n",
    "    thresh = cv2.bitwise_and(thresh, thresh, mask=roi_mask)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    thresh = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = False\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area < 1000 or area > 20000:\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        aspect_ratio = h / float(w + 1)\n",
    "\n",
    "        if aspect_ratio < 1.5 or aspect_ratio > 3.0:\n",
    "            continue\n",
    "\n",
    "        cx, cy = x + w // 2, y + h // 2\n",
    "        if roi_mask[cy, cx] == 255:\n",
    "            box_mask = np.zeros_like(roi_mask)\n",
    "            cv2.rectangle(box_mask, (x, y), (x + w, y + h), 255, -1)\n",
    "            intersection = cv2.bitwise_and(box_mask, roi_mask)\n",
    "            overlap_area = cv2.countNonZero(intersection)\n",
    "            box_area = w * h\n",
    "\n",
    "            if overlap_area / box_area > 0.7:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                detected = True\n",
    "\n",
    "    previous_frame = blur\n",
    "    return frame, detected"
   ],
   "id": "8306e3f6f4ee447d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------- Lane State Machine Class --------------------\n",
    "\n",
    "class LaneStateMachine:\n",
    "    def _init_(self, stability_threshold=5):\n",
    "        self.state = \"No Lane Detected\"\n",
    "        self.counters = defaultdict(int)\n",
    "        self.threshold = stability_threshold\n",
    "\n",
    "    def update(self, new_state):\n",
    "        for key in self.counters:\n",
    "            if key != new_state:\n",
    "                self.counters[key] = 0\n",
    "        self.counters[new_state] += 1\n",
    "\n",
    "        if self.counters[new_state] >= self.threshold:\n",
    "            if new_state != self.state:\n",
    "                self.state = new_state\n",
    "                for key in self.counters:\n",
    "                    self.counters[key] = 0\n",
    "        return self.state\n",
    "\n",
    "# -------------------- Utility Functions --------------------\n",
    "\n",
    "def apply_clahe(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    cl = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8)).apply(l)\n",
    "    return cv2.cvtColor(cv2.merge((cl, a, b)), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def detect_lanes(frame, crop_margin_percent):\n",
    "    height, width = frame.shape[:2]\n",
    "    crop_margin_x = int(width * crop_margin_percent)\n",
    "    crop_margin_y = int(height * 0.5)  # Half of the height for the ROI\n",
    "\n",
    "    roi = frame[crop_margin_y:, crop_margin_x:width - crop_margin_x]\n",
    "\n",
    "    # Apply CLAHE for contrast enhancement\n",
    "    clahe_roi = apply_clahe(roi)\n",
    "\n",
    "    # HSV yellow mask with tightened thresholds to reduce grass\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    # Bright yellow filtering\n",
    "    hsv_yellow_mask = cv2.inRange(hsv, np.array([20, 100, 100]), np.array([35, 255, 255]))\n",
    "    v_mask = cv2.inRange(v, 150, 255)\n",
    "    hsv_yellow_mask = cv2.bitwise_and(hsv_yellow_mask, hsv_yellow_mask, mask=v_mask)\n",
    "\n",
    "    # LAB yellow mask\n",
    "    lab = cv2.cvtColor(roi, cv2.COLOR_BGR2LAB)\n",
    "    lab_yellow_mask = cv2.inRange(lab, np.array([150, 120, 120]), np.array([255, 160, 160]))\n",
    "\n",
    "    # Combine HSV + LAB masks\n",
    "    combined_mask = cv2.bitwise_or(hsv_yellow_mask, lab_yellow_mask)\n",
    "\n",
    "    # Morphological filtering\n",
    "    kernel = np.ones((7, 7), np.uint8)\n",
    "    combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Edge detection\n",
    "    masked_roi = cv2.bitwise_and(clahe_roi, clahe_roi, mask=combined_mask)\n",
    "    gray = cv2.cvtColor(masked_roi, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blur, 100, 200)\n",
    "    edges = cv2.bitwise_and(edges, combined_mask)\n",
    "\n",
    "    # Detect lines\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=50, minLineLength=50, maxLineGap=30)\n",
    "    return combined_mask, edges, lines, roi\n",
    "\n",
    "def classify_lane_lines(lines, crop_margin_percent, height):\n",
    "    left_lines, right_lines = [], []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            x1 += int(crop_margin_percent * 960)\n",
    "            x2 += int(crop_margin_percent * 960)\n",
    "            y1 += height // 2\n",
    "            y2 += height // 2\n",
    "\n",
    "            if x2 - x1 == 0:\n",
    "                continue\n",
    "\n",
    "            slope = (y2 - y1) / (x2 - x1)\n",
    "            angle = np.degrees(np.arctan(abs(slope)))\n",
    "\n",
    "            if 30 < angle < 80:  # Typical lane angles\n",
    "                if slope < 0:\n",
    "                    left_lines.extend([(x1, y1), (x2, y2)])\n",
    "                else:\n",
    "                    right_lines.extend([(x1, y1), (x2, y2)])\n",
    "    return left_lines, right_lines\n",
    "\n",
    "def shade_lane_region(frame, left_lines, right_lines):\n",
    "    height = frame.shape[0]\n",
    "    overlay = np.zeros_like(frame)\n",
    "\n",
    "    if left_lines and right_lines:\n",
    "        left_points = np.array(left_lines).reshape(-1, 2)\n",
    "        right_points = np.array(right_lines).reshape(-1, 2)\n",
    "\n",
    "        left_fit = np.polyfit(left_points[:, 1], left_points[:, 0], 1)\n",
    "        right_fit = np.polyfit(right_points[:, 1], right_points[:, 0], 1)\n",
    "\n",
    "        plot_y = np.linspace(height // 2, height - 1, num=height // 2)\n",
    "        left_x = left_fit[0] * plot_y + left_fit[1]\n",
    "        right_x = right_fit[0] * plot_y + right_fit[1]\n",
    "\n",
    "        pts_left = np.array([np.transpose(np.vstack([left_x, plot_y]))])\n",
    "        pts_right = np.array([np.flipud(np.transpose(np.vstack([right_x, plot_y])))])\n",
    "        pts = np.hstack((pts_left, pts_right)).astype(np.int32)\n",
    "\n",
    "        cv2.fillPoly(overlay, [pts], (0, 255, 255))  # Yellow fill\n",
    "\n",
    "    return overlay\n",
    "\n",
    "# -------------------- Main Function --------------------\n",
    "\n",
    "def main(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    lane_state_machine = LaneStateMachine(stability_threshold=5)\n",
    "    crop_margin_percent = 0.2 # 15% of width for left/right cropping\n",
    "    obstacle_crop_margin_percent = 0.35  # 35% for obstacle detection cropping\n",
    "\n",
    "    prev_time, frame_count, fps = time.time(), 0, 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (960, 540))\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # Lane Detection\n",
    "        lane_mask, edges, lines, lane_roi = detect_lanes(frame, crop_margin_percent)\n",
    "        left_lines, right_lines = classify_lane_lines(lines, crop_margin_percent, height)\n",
    "        lane_overlay = shade_lane_region(frame, left_lines, right_lines)\n",
    "\n",
    "        # Obstacle Detection (if required)\n",
    "        obstacle_detected = False  # Remove this function if not needed\n",
    "\n",
    "        # Decision Making\n",
    "        if obstacle_detected:\n",
    "            current_decision = \"STOP - Obstacle Ahead\"\n",
    "        elif not left_lines and not right_lines:\n",
    "            current_decision = \"No Lane Detected\"\n",
    "        else:\n",
    "            current_decision = \"Go Straight\"\n",
    "\n",
    "        decision = lane_state_machine.update(current_decision)\n",
    "\n",
    "        # Overlay display\n",
    "        combined_lower = cv2.addWeighted(lane_roi, 0.7, cv2.cvtColor(lane_mask, cv2.COLOR_GRAY2BGR), 0.3, 0)\n",
    "        combined_lower = cv2.addWeighted(combined_lower, 0.9, cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR), 0.4, 0)\n",
    "        full_frame = frame.copy()\n",
    "        full_frame[height // 2:, int(width * crop_margin_percent):width - int(width * crop_margin_percent)] = combined_lower\n",
    "        output = cv2.addWeighted(full_frame, 1, lane_overlay, 0.6, 0)\n",
    "        cv2.rectangle(output, (20, 20), (620, 80), (0, 0, 0), -1)\n",
    "        cv2.putText(output, decision, (30, 65), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)\n",
    "\n",
    "        frame_count += 1\n",
    "        if frame_count >= 10:\n",
    "            curr_time = time.time()\n",
    "            fps = frame_count / (curr_time - prev_time)\n",
    "            prev_time = curr_time\n",
    "            frame_count = 0\n",
    "\n",
    "        cv2.putText(output, f\"FPS: {fps:.2f}\", (700, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        # Show windows\n",
    "        cv2.imshow('Original Frame', frame)\n",
    "        cv2.imshow('Lane Area', lane_overlay)\n",
    "        cv2.imshow('Final Output', output)\n",
    "        cv2.imshow(\"Canny Edge Output\",edges)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"PXL_20250325_043922504.TS.mp4\"\n",
    "\n",
    "    main(video_path)"
   ],
   "id": "d979298edac08443"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f8f553634284a559"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "72f91cf741ee0b53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "655f2e41e35bcccf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1b50f881508fb0b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a24ee4ca4f9fcaf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d507f086c996c80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:20:52.306081Z",
     "start_time": "2025-05-09T14:19:59.355222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to your video file\n",
    "video_path = \"PXL_20250325_043754655.TS.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "# For FPS calculation\n",
    "prev_time = time.time()\n",
    "frame_count = 0\n",
    "fps = 0\n",
    "\n",
    "# Initialize background subtractor\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Lane detection parameters (keep these as in your code)\n",
    "def is_dashed(lines, threshold=20):\n",
    "    if len(lines) < 4:\n",
    "        return False\n",
    "    lines = sorted(lines, key=lambda p: p[1])  # sort by y\n",
    "    gaps = []\n",
    "    for i in range(0, len(lines) - 2, 2):\n",
    "        y1 = lines[i][1]\n",
    "        y2 = lines[i + 2][1]\n",
    "        gaps.append(abs(y2 - y1))\n",
    "    if len(gaps) == 0:\n",
    "        return False\n",
    "    avg_gap = sum(gaps) / len(gaps)\n",
    "    return avg_gap > threshold\n",
    "\n",
    "class LaneStateMachine:\n",
    "    def __init__(self, stability_threshold=5):\n",
    "        self.state = \"No Lane Detected\"\n",
    "        self.counters = defaultdict(int)\n",
    "        self.threshold = stability_threshold\n",
    "\n",
    "    def update(self, new_state):\n",
    "        for key in self.counters:\n",
    "            if key != new_state:\n",
    "                self.counters[key] = 0\n",
    "\n",
    "        self.counters[new_state] += 1\n",
    "\n",
    "        if self.counters[new_state] >= self.threshold:\n",
    "            if new_state != self.state:\n",
    "                self.state = new_state\n",
    "                for key in self.counters:\n",
    "                    self.counters[key] = 0\n",
    "        return self.state\n",
    "\n",
    "# Initialize the state machine\n",
    "lane_state_machine = LaneStateMachine(stability_threshold=4)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (960, 540))\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Detect yellow lanes to define road area\n",
    "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    lower_yellow = np.array([15, 30, 30])  # Lower bound of yellow color\n",
    "    upper_yellow = np.array([40, 255, 255])  # Upper bound of yellow color\n",
    "    yellow_mask = cv2.inRange(hsv_frame, lower_yellow, upper_yellow)\n",
    "\n",
    "    # Use morphological operations to clean up the yellow lane mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    yellow_mask = cv2.morphologyEx(yellow_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    yellow_mask = cv2.morphologyEx(yellow_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Find contours in the yellow lane mask to get lane boundaries\n",
    "    contours, _ = cv2.findContours(yellow_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    lane_contours = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 500:  # Threshold to avoid small noise contours\n",
    "            lane_contours.append(cnt)\n",
    "\n",
    "    # Create a mask for the road area (only between yellow lanes)\n",
    "    road_mask = np.zeros_like(frame[:, :, 0])  # Single channel mask\n",
    "    cv2.drawContours(road_mask, lane_contours, -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "    # Initialize a grey road area (start with black image)\n",
    "    grey_road = np.zeros_like(frame)\n",
    "    grey_road[:] = [0, 0, 0]  # Start with black color\n",
    "\n",
    "    # Detect the first and second yellow lanes\n",
    "    lane_positions = []\n",
    "    for cnt in lane_contours:\n",
    "        # Find the bounding box of each lane and record its horizontal position\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        lane_positions.append(x)\n",
    "\n",
    "    # Sort the lane positions from left to right\n",
    "    lane_positions.sort()\n",
    "\n",
    "    if len(lane_positions) >= 2:  # Check if both lanes are detected\n",
    "        left_lane_x = lane_positions[0]\n",
    "        right_lane_x = lane_positions[1]\n",
    "\n",
    "        # Define the region between the yellow lanes\n",
    "        grey_road[:, left_lane_x:right_lane_x] = [128, 128, 128]  # Fill the area between lanes with grey\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Object Detection using Background Subtraction\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_CLOSE, kernel)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Apply the road mask to limit detection to the road area (everything outside the road is black)\n",
    "    fgmask = cv2.bitwise_and(fgmask, fgmask, mask=road_mask)\n",
    "\n",
    "    # Find contours for objects detected on the road\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    obstacle_detected = False\n",
    "\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 500 and area < 10000:  # Threshold for object size\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            center_x = x + w // 2\n",
    "            x_global = x\n",
    "            center_x_global = center_x\n",
    "\n",
    "            # Only consider objects within the road area (between yellow lanes)\n",
    "            if (width // 4 < center_x_global < 3 * width // 4):\n",
    "                cv2.rectangle(frame, (x_global, y), (x_global + w, y + h), (0, 0, 255), 2)\n",
    "                cv2.putText(frame, \"Obstacle Detected\", (x_global, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "                obstacle_detected = True\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Decision Making - Raw State\n",
    "    if obstacle_detected:\n",
    "        current_decision = \"STOP - Obstacle Ahead\"\n",
    "    else:\n",
    "        current_decision = \"Go Straight\"\n",
    "\n",
    "    # Use state machine to stabilize the decision\n",
    "    direction_text = lane_state_machine.update(current_decision)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Final Display\n",
    "    output = frame.copy()\n",
    "\n",
    "    # Show the direction decision on the output\n",
    "    cv2.rectangle(output, (20, 20), (620, 80), (0, 0, 0), -1)\n",
    "    cv2.putText(output, direction_text, (30, 65), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)\n",
    "\n",
    "    # Show FPS on the output\n",
    "    frame_count += 1\n",
    "    if frame_count >= 10:\n",
    "        curr_time = time.time()\n",
    "        fps = frame_count / (curr_time - prev_time)\n",
    "        prev_time = curr_time\n",
    "        frame_count = 0\n",
    "\n",
    "    cv2.putText(output, f\"FPS: {fps:.2f}\", (700, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "    # Display grey road area in Lane Area window\n",
    "    cv2.imshow('Lane Area', grey_road)\n",
    "\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Final Output', output)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "16aa77bdf4ebb4da",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "56c00e2ad009937b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7cf968344313e70d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "77a1a90b6955988e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1bc3b1f1397f0b00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "710f2f3ac2a45d1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e4796d2e23b991f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "82f339c0faac7c9c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T13:26:30.988540Z",
     "start_time": "2025-05-09T13:26:30.933573Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "# Path to your video file\n",
    "video_path = \"PXL_20250325_043754655.TS.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "# For FPS calculation\n",
    "prev_time = time.time()\n",
    "frame_count = 0\n",
    "fps = 0\n",
    "\n",
    "def is_dashed(lines, threshold=20):\n",
    "    if len(lines) < 4:\n",
    "        return False\n",
    "    lines = sorted(lines, key=lambda p: p[1])  # sort by y\n",
    "    gaps = []\n",
    "    for i in range(0, len(lines) - 2, 2):\n",
    "        y1 = lines[i][1]\n",
    "        y2 = lines[i + 2][1]\n",
    "        gaps.append(abs(y2 - y1))\n",
    "    if len(gaps) == 0:\n",
    "        return False\n",
    "    avg_gap = sum(gaps) / len(gaps)\n",
    "    return avg_gap > threshold\n",
    "from collections import defaultdict\n",
    "\n",
    "class LaneStateMachine:\n",
    "    def __init__(self, stability_threshold=5):\n",
    "        self.state = \"No Lane Detected\"\n",
    "        self.counters = defaultdict(int)\n",
    "        self.threshold = stability_threshold\n",
    "\n",
    "    def update(self, new_state):\n",
    "        # Reset all counters except the current suggestion\n",
    "        for key in self.counters:\n",
    "            if key != new_state:\n",
    "                self.counters[key] = 0\n",
    "\n",
    "        self.counters[new_state] += 1\n",
    "\n",
    "        # If new_state is consistent for N frames, update state\n",
    "        if self.counters[new_state] >= self.threshold:\n",
    "            if new_state != self.state:\n",
    "                self.state = new_state\n",
    "                # Reset all counters after transition\n",
    "                for key in self.counters:\n",
    "                    self.counters[key] = 0\n",
    "        return self.state\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T13:27:33.901503Z",
     "start_time": "2025-05-09T13:26:32.945988Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "dfc303d6634421a5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T13:56:12.964926Z",
     "start_time": "2025-05-09T13:55:39.642664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('PXL_20250325_043754655.TS.mp4')\n",
    "\n",
    "# Check if the video file was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file\")\n",
    "    exit()\n",
    "\n",
    "# Loop through each frame in the video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to read frame\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise and improve edge detection\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Use Canny edge detection to find edges in the image\n",
    "\n",
    "    edges = cv2.Canny(blurred, 100, 200)\n",
    "\n",
    "    # Find contours from the edges\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Iterate over the contours and filter out small ones\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Filter out small contours\n",
    "            # Get the bounding box for each detected object\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "            # Draw a rectangle around the detected object\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Optionally, show the Canny edge image and the original frame\n",
    "    cv2.imshow('Edges', edges)\n",
    "    cv2.imshow('Detected Objects', frame)\n",
    "\n",
    "    # Check for key press to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "4bfcc4cd05a380af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to read frame\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T09:57:01.007340Z",
     "start_time": "2025-05-08T09:56:35.976619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('PXL_20250325_043754655.TS.mp4')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to read frame\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Canny edge detection\n",
    "    edges = cv2.Canny(blurred, 100, 200)\n",
    "\n",
    "    # Define Region of Interest (ROI) for lane detection (focus on the road)\n",
    "    height, width = edges.shape\n",
    "    roi_vertices = [(0, height), (width // 2, height // 2), (width, height)]\n",
    "    mask = np.zeros_like(edges)\n",
    "    cv2.fillPoly(mask, np.array([roi_vertices], np.int32), 255)\n",
    "    roi_edges = cv2.bitwise_and(edges, mask)\n",
    "\n",
    "    # Apply Hough Transform to detect lanes\n",
    "    lines = cv2.HoughLinesP(roi_edges, 1, np.pi / 180, threshold=50, minLineLength=100, maxLineGap=200)\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "    # Show the result\n",
    "    cv2.imshow('Detected Lanes', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "ef7f3b05309028ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to read frame\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:02:40.714255Z",
     "start_time": "2025-05-09T14:02:06.356491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_lanes_and_objects(frame):\n",
    "    # Resize for consistency\n",
    "    frame = cv2.resize(frame, (960, 540))\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Canny edge detection\n",
    "    edges = cv2.Canny(blur, 50, 150)\n",
    "\n",
    "    # Define region of interest (trapezoid shape for lane area)\n",
    "    mask = np.zeros_like(edges)\n",
    "    h, w = edges.shape\n",
    "    roi_corners = np.array([[\n",
    "        (w//10, h),\n",
    "        (w//2 - 50, h//2 + 40),\n",
    "        (w//2 + 50, h//2 + 40),\n",
    "        (w - w//10, h)\n",
    "    ]], dtype=np.int32)\n",
    "    cv2.fillPoly(mask, roi_corners, 255)\n",
    "    masked_edges = cv2.bitwise_and(edges, mask)\n",
    "\n",
    "    # Hough Line Transform for lane detection\n",
    "    lines = cv2.HoughLinesP(masked_edges, 1, np.pi/180, 50, minLineLength=50, maxLineGap=100)\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 255), 3)\n",
    "\n",
    "    # Create lane polygon for object-in-lane check\n",
    "    lane_polygon = roi_corners[0]\n",
    "\n",
    "    # Object detection using thresholding and CCA\n",
    "    _, binary = cv2.threshold(blur, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Connected Component Analysis\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(closed)\n",
    "\n",
    "    object_detected = False\n",
    "    object_on_lane = False\n",
    "\n",
    "    for i in range(1, num_labels):  # Skip background\n",
    "        area = stats[i, cv2.CC_STAT_AREA]\n",
    "        if area > 500:\n",
    "            x = stats[i, cv2.CC_STAT_LEFT]\n",
    "            y = stats[i, cv2.CC_STAT_TOP]\n",
    "            w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "            h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            cx, cy = centroids[i]\n",
    "\n",
    "            object_detected = True\n",
    "            if cv2.pointPolygonTest(lane_polygon, (int(cx), int(cy)), False) >= 0:\n",
    "                object_on_lane = True\n",
    "                color = (0, 0, 255)  # Red for objects on lane\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for objects off lane\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.circle(frame, (int(cx), int(cy)), 3, (255, 0, 0), -1)\n",
    "\n",
    "    # Display detection result\n",
    "    cv2.putText(frame, f'Object: {\"Detected\" if object_detected else \"Not\"}', (20, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    cv2.putText(frame, f'On Lane: {\"Yes\" if object_on_lane else \"No\"}', (20, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    return frame, masked_edges\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Main Execution (example for video)\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    cap = cv2.VideoCapture(\"PXL_20250325_043754655.TS.mp4\")  # Replace with your filename\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        result_frame, edges = detect_lanes_and_objects(frame)\n",
    "\n",
    "        cv2.imshow(\"Edges\", edges)\n",
    "        cv2.imshow(\"Detected Lanes & Objects\", result_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ],
   "id": "14e0b308de7130ef",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 88\u001B[39m\n\u001B[32m     85\u001B[39m result_frame, edges = detect_lanes_and_objects(frame)\n\u001B[32m     87\u001B[39m cv2.imshow(\u001B[33m\"\u001B[39m\u001B[33mEdges\u001B[39m\u001B[33m\"\u001B[39m, edges)\n\u001B[32m---> \u001B[39m\u001B[32m88\u001B[39m \u001B[43mcv2\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimshow\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mDetected Lanes & Objects\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresult_frame\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cv2.waitKey(\u001B[32m1\u001B[39m) & \u001B[32m0xFF\u001B[39m == \u001B[38;5;28mord\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mq\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m     91\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fb55ec11776e4ded"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6d787b6924f7aad2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c67c08af5d7fa5e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "be6136179ed6541"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b8074fbc11bbbe02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b5590de555692eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2ca968807235518c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5693c7a35e29e1ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9d158598cc5212a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:23:53.581200Z",
     "start_time": "2025-05-07T19:23:51.743550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def sobel_filter(img):\n",
    "    \"\"\"Apply Sobel operator to calculate gradients in X and Y directions.\"\"\"\n",
    "    sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "    sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
    "\n",
    "    grad_x = cv2.filter2D(img, -1, sobel_x)\n",
    "    grad_y = cv2.filter2D(img, -1, sobel_y)\n",
    "\n",
    "    return grad_x, grad_y\n",
    "\n",
    "def non_maximum_suppression(grad_magnitude, grad_direction):\n",
    "    \"\"\"Apply non-maximum suppression to thin out edges.\"\"\"\n",
    "    rows, cols = grad_magnitude.shape\n",
    "    output = np.zeros_like(grad_magnitude)\n",
    "\n",
    "    for i in range(1, rows - 1):\n",
    "        for j in range(1, cols - 1):\n",
    "            # Determine edge direction and round to nearest 45 degrees\n",
    "            angle = grad_direction[i, j]\n",
    "            if angle < 0:\n",
    "                angle += 180\n",
    "            if (0 <= angle < 22.5) or (157.5 <= angle < 180):\n",
    "                neighbor1 = grad_magnitude[i, j + 1]\n",
    "                neighbor2 = grad_magnitude[i, j - 1]\n",
    "            elif (22.5 <= angle < 67.5):\n",
    "                neighbor1 = grad_magnitude[i + 1, j - 1]\n",
    "                neighbor2 = grad_magnitude[i - 1, j + 1]\n",
    "            elif (67.5 <= angle < 112.5):\n",
    "                neighbor1 = grad_magnitude[i + 1, j]\n",
    "                neighbor2 = grad_magnitude[i - 1, j]\n",
    "            elif (112.5 <= angle < 157.5):\n",
    "                neighbor1 = grad_magnitude[i - 1, j - 1]\n",
    "                neighbor2 = grad_magnitude[i + 1, j + 1]\n",
    "\n",
    "            # Suppress non-maximum pixels\n",
    "            if grad_magnitude[i, j] >= neighbor1 and grad_magnitude[i, j] >= neighbor2:\n",
    "                output[i, j] = grad_magnitude[i, j]\n",
    "            else:\n",
    "                output[i, j] = 0\n",
    "\n",
    "    return output\n",
    "\n",
    "def double_thresholding(img, low_threshold, high_threshold):\n",
    "    \"\"\"Apply double thresholding to classify pixels as strong, weak, or non-edges.\"\"\"\n",
    "    strong_edges = np.zeros_like(img)\n",
    "    weak_edges = np.zeros_like(img)\n",
    "\n",
    "    strong_edges[img > high_threshold] = 255\n",
    "    weak_edges[(img >= low_threshold) & (img <= high_threshold)] = 255\n",
    "\n",
    "    return strong_edges, weak_edges\n",
    "\n",
    "def edge_tracking_by_hysteresis(strong_edges, weak_edges):\n",
    "    \"\"\"Track edges by hysteresis (connect weak edges to strong edges).\"\"\"\n",
    "    rows, cols = strong_edges.shape\n",
    "    final_edges = np.copy(strong_edges)\n",
    "\n",
    "    for i in range(1, rows - 1):\n",
    "        for j in range(1, cols - 1):\n",
    "            if weak_edges[i, j] == 255:\n",
    "                # Check 8 neighbors to connect weak edges to strong edges\n",
    "                if np.any(strong_edges[i - 1:i + 2, j - 1:j + 2] == 255):\n",
    "                    final_edges[i, j] = 255\n",
    "                else:\n",
    "                    final_edges[i, j] = 0\n",
    "\n",
    "    return final_edges\n",
    "\n",
    "def manual_canny_edge_detection(img, low_threshold=100, high_threshold=200):\n",
    "    \"\"\"Manual Canny edge detection implementation.\"\"\"\n",
    "    # Step 1: Apply Gaussian Blur\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    # Step 2: Calculate gradients using Sobel filters\n",
    "    grad_x, grad_y = sobel_filter(blurred)\n",
    "\n",
    "    # Step 3: Calculate gradient magnitude and direction\n",
    "    grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    grad_direction = np.arctan2(grad_y, grad_x) * 180 / np.pi\n",
    "\n",
    "    # Step 4: Apply non-maximum suppression\n",
    "    non_max_suppressed = non_maximum_suppression(grad_magnitude, grad_direction)\n",
    "\n",
    "    # Step 5: Apply double thresholding\n",
    "    strong_edges, weak_edges = double_thresholding(non_max_suppressed, low_threshold, high_threshold)\n",
    "\n",
    "    # Step 6: Perform edge tracking by hysteresis\n",
    "    final_edges = edge_tracking_by_hysteresis(strong_edges, weak_edges)\n",
    "\n",
    "    return final_edges\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('PXL_20250325_043754655.TS.mp4')\n",
    "\n",
    "# Check if the video file was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file\")\n",
    "    exit()\n",
    "\n",
    "# Loop through each frame in the video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to read frame\")\n",
    "        break\n",
    "\n",
    "    # Resize frame for easier processing (optional)\n",
    "    frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply manual Canny edge detection\n",
    "    edges = manual_canny_edge_detection(gray, low_threshold=100, high_threshold=200)\n",
    "\n",
    "    # Ensure the edge image is in the correct format for contours (uint8, values 0 or 255)\n",
    "    edges = np.uint8(edges)\n",
    "\n",
    "    # Find contours from the edges\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Iterate over the contours and filter out small ones\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Filter out small contours\n",
    "            # Get the bounding box for each detected object\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "            # Draw a rectangle around the detected object\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Optionally, show the edge image and the original frame\n",
    "    cv2.imshow('Edges', edges)\n",
    "    cv2.imshow('Detected Objects', frame)\n",
    "\n",
    "    # Check for key press to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "73e1a570b10c1ab5",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 118\u001B[39m\n\u001B[32m    115\u001B[39m gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\u001B[32m    117\u001B[39m \u001B[38;5;66;03m# Apply manual Canny edge detection\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m edges = \u001B[43mmanual_canny_edge_detection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_threshold\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh_threshold\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    120\u001B[39m \u001B[38;5;66;03m# Ensure the edge image is in the correct format for contours (uint8, values 0 or 255)\u001B[39;00m\n\u001B[32m    121\u001B[39m edges = np.uint8(edges)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 85\u001B[39m, in \u001B[36mmanual_canny_edge_detection\u001B[39m\u001B[34m(img, low_threshold, high_threshold)\u001B[39m\n\u001B[32m     82\u001B[39m grad_direction = np.arctan2(grad_y, grad_x) * \u001B[32m180\u001B[39m / np.pi\n\u001B[32m     84\u001B[39m \u001B[38;5;66;03m# Step 4: Apply non-maximum suppression\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m non_max_suppressed = \u001B[43mnon_maximum_suppression\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad_magnitude\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_direction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     87\u001B[39m \u001B[38;5;66;03m# Step 5: Apply double thresholding\u001B[39;00m\n\u001B[32m     88\u001B[39m strong_edges, weak_edges = double_thresholding(non_max_suppressed, low_threshold, high_threshold)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 42\u001B[39m, in \u001B[36mnon_maximum_suppression\u001B[39m\u001B[34m(grad_magnitude, grad_direction)\u001B[39m\n\u001B[32m     40\u001B[39m             output[i, j] = grad_magnitude[i, j]\n\u001B[32m     41\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m             output[i, j] = \u001B[32m0\u001B[39m\n\u001B[32m     44\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:12:11.637199Z",
     "start_time": "2025-05-08T10:11:48.376674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the input image\n",
    "image = cv2.imread('Fig01.tif')  # Replace 'your_image.jpg' with the path to your image\n",
    "\n",
    "# Resize the image for easier visualization (optional)\n",
    "image = cv2.resize(image, (960, 540))\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply GaussianBlur to reduce noise and improve edge detection\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "# Detect edges using Canny edge detector\n",
    "edges = cv2.Canny(blurred, 100, 200)\n",
    "\n",
    "# Apply Hough Line Transform to detect straight lines\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=50, minLineLength=100, maxLineGap=200)\n",
    "\n",
    "# If lines are detected, draw them on the original image\n",
    "if lines is not None:\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        cv2.line(image, (x1, y1), (x2, y2), (0, 255, 0), 3)  # Draw green lines\n",
    "\n",
    "# Display the original image with highlighted lines\n",
    "cv2.imshow('Detected Lines', image)\n",
    "\n",
    "# Wait for a key press and close all windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "d0dd8d7018aad024",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T06:43:54.218975Z",
     "start_time": "2025-05-11T06:43:34.182611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained Haar Cascade classifier for detecting humans (body)\n",
    "object_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_fullbody.xml')\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('PXL_20250325_043754655.TS.mp4')  # Replace with your video file path\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to read frame\")\n",
    "        break\n",
    "\n",
    "\n",
    "    frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    objs = object_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    for (x, y, w, h) in objs:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)  # Green rectangle around human\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "af2c6ffca2c8cd85",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:35:26.696325Z",
     "start_time": "2025-05-08T19:35:02.550370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video file (you can use 'your_video.mp4' or an image path)\n",
    "cap = cv2.VideoCapture('PXL_20250325_043754655.TS.mp4')\n",
    "# Check if the video was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to read frame\")\n",
    "        break\n",
    "\n",
    "    # Resize the frame for easier processing (optional)\n",
    "    frame = cv2.resize(frame, (960, 540))\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise and improve edge detection\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Step 1: Detect edges using Canny edge detector\n",
    "    edges = cv2.Canny(blurred, 100, 200)\n",
    "\n",
    "    # Step 2: Detect lines using Hough Line Transform\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=50, minLineLength=100, maxLineGap=200)\n",
    "\n",
    "    # Step 3: Draw the detected lines\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)  # Green lines\n",
    "\n",
    "    # Step 4: Detect circles using Hough Circle Transform\n",
    "    circles = cv2.HoughCircles(blurred, cv2.HOUGH_GRADIENT, dp=1, minDist=30, param1=50, param2=30, minRadius=10, maxRadius=100)\n",
    "\n",
    "    # Step 5: Draw the detected circles\n",
    "    if circles is not None:\n",
    "        circles = np.round(circles[0, :]).astype(\"int\")\n",
    "        for (x, y, r) in circles:\n",
    "            cv2.circle(frame, (x, y), r, (0, 0, 255), 3)  # Red circles\n",
    "            cv2.rectangle(frame, (x - 5, y - 5), (x + 5, y + 5), (0, 128, 255), -1)  # Small rectangle at the center\n",
    "\n",
    "    # Display the processed frame with detected objects\n",
    "    cv2.imshow('Detected Objects (Lines & Circles)', frame)\n",
    "\n",
    "    # Wait for key press to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "f90e2605ee4d1c5",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[32m     37\u001B[39m         cv2.line(frame, (x1, y1), (x2, y2), (\u001B[32m0\u001B[39m, \u001B[32m255\u001B[39m, \u001B[32m0\u001B[39m), \u001B[32m3\u001B[39m)  \u001B[38;5;66;03m# Green lines\u001B[39;00m\n\u001B[32m     39\u001B[39m \u001B[38;5;66;03m# Step 4: Detect circles using Hough Circle Transform\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m circles = \u001B[43mcv2\u001B[49m\u001B[43m.\u001B[49m\u001B[43mHoughCircles\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblurred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv2\u001B[49m\u001B[43m.\u001B[49m\u001B[43mHOUGH_GRADIENT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdp\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mminDist\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam1\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam2\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mminRadius\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaxRadius\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[38;5;66;03m# Step 5: Draw the detected circles\u001B[39;00m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m circles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9b61278165cce0bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
